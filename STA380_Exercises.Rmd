---
title: "STA-380 Exercises"
header-includes:
 \usepackage{float}
 \usepackage{lscape}
output: 
  pdf_document:
    extra_dependencies: ["float"]
date: "2022-08-10"
author: "Khyati Jariwala, Aishwarya Rajeev, Juhi Patel, Varun Kausika"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
knitr::opts_chunk$set(echo = FALSE)
```

\newcommand{\blandscape}{\begin{landscape}}
\newcommand{\elandscape}{\end{landscape}}

\section{1. Probability practice}

\subsection{Part A}

**Given information:** 

Two categories of users: 

1. Truthful clicker (TC) 
2. Random clicker (RC)

**Information on probablities:**

\begin{itemize}
\item $P(RC) = 0.3$
\item $P(Yes|RC) = 0.5$
\item $P(No|RC) = 0.5$
\item $P(TC) = 0.7$
\item $P(Yes|TC) = x$
\item $P(No|TC) = 1-x$
\item $P(Yes) = 0.65$
\item $P(No) = 0.35$
\end{itemize}

*Using the Rule of Total Probability*, 

\begin{equation}
P(Yes) = P(Yes, TC) + P(Yes, RC) = P(TC)*P(Yes|TC) + P(RC)*P(Yes|RC)
\label{eqn:A_1}
\end{equation}

\begin{equation*}
P(Yes) = 0.7x + 0.3*0.5 = 0.7x + 0.15 = 0.65
\end{equation*}

Solving for x, we get, 

**$x = P(Yes|TC) = 0.714$** 

\subsection{Part B}

We are being asked $P(Diseased|Positive)$

**Given information:**

\begin{itemize}
\item $P(Positive|Diseased) = 0.993$
\item $P(Negative|Not Diseased) = 0.9999$
\item $P(Diseased) = 0.000025$
\end{itemize}

*According to Bayes Rule and Rule of Total Probability*,

\begin{equation}
P(Diseased|Positive) = \frac{P(Positive|Diseased)*P(Diseased)}{P(Positive)}
\label{eqn:B_1}
\end{equation}

and, 

\begin{equation}
P(Positive) = P(Positive|Diseased)*P(Diseased) + P(Positive|Not\:Diseased)*P(Not\:Diseased)
\label{eqn:B_2}
\end{equation}

Therefore, 

\begin{equation*}
P(Positive) = 0.993*0.000025 + 0.0001*0.999975 = 0.000125
\end{equation*}

Substituting in \eqref{eqn:B_1} we get, 

\begin{equation*}
P(Diseased|Positive) = \frac{0.993*0.000025}{0.000125} = \boldmath{0.1986}
\end{equation*}

\section{2. Wrangling the Billboard Top 100}

\subsection{Part A}

**First, we load in the data and perform a group by on the performer and the song, with an agg function of count for the week**

```{r, echo=FALSE}
df = read.csv('billboard.csv')
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)

grouped_df = df %>% group_by(performer, song) %>% summarize_at(vars(week), funs(length)) 
grouped_df = grouped_df %>% rename(count=week)
knitr::kable(head(grouped_df, 10), caption="Billboards")
```

**Finally, we sort the dataframe in descending order of counts and find the top 10 and give our table a caption:**

```{r, echo= FALSE}
top_10 = grouped_df[order(grouped_df$count, decreasing=TRUE),][1:10,]
```

```{r, echo=FALSE}
knitr::kable(top_10, caption="Top 10 most popular songs")
```

\subsection{Part B}

First we group by year and order by ascending year. Then, we remove the years 1958 and 2021 from the rows and order just to make sure. Finally, we proceed to plot the columns.

```{r, echo=FALSE}
grouped_df_2 = df %>% group_by(year) %>% summarise(n_distinct(song))
grouped_df_2 = grouped_df_2[grouped_df_2$year!= 1958, ]
grouped_df_2 = grouped_df_2[grouped_df_2$year!= 2021, ]
grouped_df_2 = grouped_df_2[order(grouped_df_2$year, decreasing=FALSE),]
```

```{r plot, fig.cap='The plot shows peaks in diversity in 1965 and 2020, along with extreme lows in 2000', echo=FALSE}
div_years = plot(grouped_df_2$year, grouped_df_2$`n_distinct(song)`, type = 'l', xlab = 'Year', ylab = 'Musical Diversity')
```
\subsection{Part C}

First, we filter the dataframe from part A to include only those songs with weeks at least 10. Then, We do a group by on the artists. Finally, we can select those artists with a hit-count of at least 30. \ref{fig:10-weekhits}
```{r, echo=FALSE}
ten_week_hits = grouped_df[grouped_df$count>=10,]
```

```{r, echo= FALSE}
grouped_df_3 = ten_week_hits %>% group_by(performer) %>% summarize_at(vars(song), funs(length))
artists_19 = grouped_df_3[grouped_df_3$song>=30, ]
```

```{r plot2, echo=FALSE, fig.cap='Elton John has more hits than others by quite a large margin'}
library(ggplot2)
ggplot(data=artists_19, aes(x=performer, y=song)) + geom_bar(stat="identity", fill="steelblue") + theme(axis.text.x=element_text(angle = -90, hjust = 0)) + labs(y = "Number of 10-week hits")
```

\section{3. Visual Storytelling Part 1: green buildings}

```{r}
df = read.csv('greenbuildings.csv')
df = select(df, -c(empl_gr))
```

\subsection{Outliers:}
We are interested in finding the potential economic gains the owner could make by constructing a 15-story green building in the neighbourhood of East Cesar Chavez.

**First, we remove the outliers from the dataset as mentioned by the analyst.**

```{r plot3, fig.cap="It seems like occupancy below 40 percent can be removed in contrast to the 10 percent that was suggested", fig.height=2}
par(mfrow = c(1, 3))
boxplot(df$leasing_rate, ylab='Occupancy')
boxplot(df$Rent, ylab='Rent')
boxplot(df$size, ylab='Size')
```

```{r, message=FALSE, results='hide'}
df[df$leasing_rate<40,]
```
**Observations:**
\begin{enumerate}
\item Removing outliers in occupancy below 40\% would only remove 456 rows from our dataset, so we proceed with this step.
\item We should not remove outliers from rent, as that would shrink our dataset greatly, and also since most of the green buildings have higher rents, this would remove them entirely.
\item We should not remove outliers from size, because size and rent are positively correlated and this might remove a significant number of green buildings as well.
\end{enumerate}


```{r}
df = df[df$leasing_rate>40,]
```

\subsection{Correlation Matrix:}

Now, we plot the feature correlation matrix in our dataset.
```{r plot4, fig.cap="As we can see, rent is highly dependent on clusters", fig.height=3.5}
library(reshape2)
library(ggplot2)
cormat = round(cor(df),2)

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd = as.dist((1-cormat)/2)
hc = hclust(dd)
cormat = cormat[hc$order, hc$order]
}

# Reorder the correlation matrix
cormat = reorder_cormat(cormat)

# Melt the correlation matrix
melted_cormat = melt(cormat, na.rm = TRUE)
# Create a ggheatmap
ggheatmap = ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Correlation") +
  theme_minimal()+ 
  theme(text = element_text(size = 10)) +
 theme(axis.text.x = element_text(angle = 90)) +
 coord_fixed()
# Print the heatmap
print(ggheatmap)
```
\textbf{Observations:}

\begin{enumerate}
\item Rent and Cluster has a high positive correlation, because some neighbourhoods are more expensive than others. So, instead of taking the median rent throughout our dataset, we can just use the cluster rent variable.
\item Rent and age has a mildly negative correlation, as one would expect.
\item Rent is not correlated with green rating, LEED or Energy Star.
\item Class A buildings are positively correlated with rent whereas Class B buildings are negatively correlated.
\item Leasing rate is positively correlated with rent.
\item Size and stories(can be considered correlated features) are both positively correlated to rent.
\end{enumerate}

\subsection{Scatter plots:}

**Now, we draw some scatter plots to understand the relation between correlated variables.**

```{r plot5, fig.cap="We can see clear patterns in the cluster rent v cluster. Some clusters have a significantly higher average rent.", fig.height=4}
par(mfrow=c(2, 3))
plot(df$size, df$Rent)
plot(df$cluster, df$cluster_rent)
plot(df$class_a, df$Rent)
plot(df$class_b, df$Rent)
plot(df$leasing_rate, df$Rent)
plot(df$size, df$Rent)
```
**Observations**
\begin{enumerate}
\item Rent in Class A buildings occur more in positive class. This could be because of the positive correlation of class A buildings with green rating, size and stories. 
\item Rent in Class B buildings occur more in negative class. The correlation of Class B with the variables is exactly opposite to that of Class A. \item This implies that the builder \textbf{MUST} aim to construct a Class A building in order to be able to charge more rent.
\item Furthermore, the builder might want to move the building location to a cluster that falls in higher average rent bracket.
\end{enumerate}

\subsection{Density plots:}

**Now, we look at the green buildings in our dataset and compare them to all the other buildings**

```{r}
greens = df[df$green_rating == 1, ]
non_greens = df[df$green_rating == 0, ]
```

```{r plot6, fig.cap="We can see that both green buildings and non-green buildings are not normally distributed. They are shifted to the left.", fig.height=3.5}
par(mfrow = c(1, 2))

plot(density(greens$Rent), lwd = 2, col = "red", main = "Density plots for rent", xlab = "", ylim=c(0, 0.05))

set.seed(2)
y = non_greens$Rent
dy = density(y)

lines(dy, col = "blue", lwd = 2)

legend("topright", legend=c("Green", "Non-green"), col = c("red", "blue"), lty=1:1, cex=0.8)

plot(density(greens$cluster_rent), lwd = 2, col = "red", main = "Density plots for cluster-rent", xlab = "", ylim=c(0, 0.06))

set.seed(2)
y = non_greens$cluster_rent
dy = density(y)

lines(dy, col = "blue", lwd = 2)

legend("topright", legend=c("Green", "Non-green"), col = c("red", "blue"), lty=1:1, cex=0.8)
```
**Observations:**

\begin{enumerate}
\item We can conclude that this data is skewed toward the lower rent buildings (has long tails).
\item When comparing green and non-green buildings, the green buildings seem to peak at a slightly higher rent than the non green rents. Comparing the median values of both of these will be appropriate for the graph on the left.
\item However, for the cluster rent graph, we see that the peak of the non-green occurs at a slightly higher rent value than the peak for the green buildings.Therefore, if we use cluster rent as our primary variable to determine income (as we said we would before), we would expect in fact a lower rent value for the greens.
\item The cluster rent density function is not normal for either the greens or the non greens. For the non-greens, it looks more like a sum of many different bell shaped curves. This suggests that each cluster within the data has a separate normal distribution.
\end{enumerate}

\subsection{Economic Impact of Green Houses:}

Economic impact could include a variety of factors in this case, namely:

\begin{enumerate}
\item \textbf{Sources of income depend on the rent being charged. Which inturn are caused by:}
\begin{enumerate}
\item Size of the house being rented (250,000 sqft in our case). 
\item The neighbourhood of the house. A groupby statement is in order to find out the prices in East Cesar Chavez. On top of this, the premium rent for being green, as shown in the density plot, is pretty much no-existent. So controlling for the clusters, we can say that green houses don't do much better than non-green houses in this dataset.
\item Age of the building. One would expect that the older the building is the lower the rent is. This is shown in the correlation plot.
\item Appreciation of value of houses in the neighbourhood. This is something that cannot be measured by the given features. A thorough analysis of previous time series data of prices is recommended.
\item Occupancy of the building (relevant variable here is lease.rate).
\item Whether amenities are available or not.
\item Whether it was renovated or not.
\item Whether it is Class A or Class B.
\end{enumerate}
\item \textbf{Sources of expenditure depend on:}
\begin{enumerate}
\item Initial capex (100,000 in our case).
\item Premium needed to be spent on constructing a green building (5\%).
\item Maintainance, repairs and other charges.
\item \textbf{NOTE: Water, Gas and Electricity charges are assumed to be paid by the tenant, hence are not included.}
\end{enumerate}
\end{enumerate}

\subsubsection{Where the Analyst went wrong:}

The analyst found the impact on income in a very linear way. He did not take into account the following:

\begin{enumerate}
\item That the green buildings do not have any premium in rent after accounting for the cluster in which the buildings are.
\item That 40\% of the occupancies are outliers, since they have a very high mean and low interquartile range.
\item Time value of money: he has calculated the simple Payback Period of the project. The more accurate estimator of success would have been the Net Present Value.
\item A cross sectional analysis across other buildings should have been done to compare Payback Periods instead of assuming 8 years is a decent time to recuperate costs
\item The worst case of occupancy is not 90\%. It is much lower. As seen in the dataset many buildings have occupancies even lower than 40\%.
\item That the cost of the buildings in the area may reduce/increase over time.
\item Accounting for other variables which may inhibit the ability to charge high rent (eg. Class A vs B, others listed above).
\end{enumerate}

\subsubsection{Confounding variables:}
```{r plot7, fig.cap="Some green houses show a high electricity and gas cost which goes against our intuition of what is green. The outliers can be adjusted for by removing them", fig.height=3}
par(mfrow=c(1, 2))
boxplot(greens$Gas_Costs, ylab = 'Gas costs', xlab='Green buildings')
boxplot(greens$Electricity_Costs, ylab = 'Electricity costs', xlab='Green buildings')
```
\section{4. Visual Storytelling part 2: Capital Metro data:}

```{r}
df = read.csv('capmetro_UT.csv')
```

```{r}
library(dplyr)
df['diff']= df$alighting - df$boarding
df_hour = df %>% group_by(hour_of_day) %>% summarize(mean_boarding = mean(boarding), mean_alighting = mean(alighting), mean_temp = mean(temperature), mean_diff = mean(diff))
```

```{r plot8, fig.cap="In the campus vicinity, alighting is most common in the morning (when classes start) and boarding is most common in the evening (when classes end). Furthermore, temperature doesn't seem to have an affect on the boardings and alightings. Here, net-inflow is calculated as alightings minus boardings. Net inflow is 0 around noon (break between classes).", fig.height=3.5}
plot(df_hour$hour_of_day, df_hour$mean_boarding, type="l",col="red", ylim = c(-80, 120), main = "Comparison of boarding vs alighting hours by hour of day", ylab='Mean number of people', xlab='Hour of day')
lines(df_hour$hour_of_day, df_hour$mean_alighting, col="blue")
lines(df_hour$hour_of_day, df_hour$mean_temp, col="orange")
lines(df_hour$hour_of_day, df_hour$mean_diff, col="green")
abline(h=0)
legend("topright", legend=c("Boarding", "Alighting", "Temperature", "Net-Inflow"), col = c("red", "blue", "orange", "green"), lty=1:1, cex=0.8)

```
```{r plot9, fig.cap="Similar patterns are seen across months, temperatures are highest in September and gradually come down", fig.height=3, message=FALSE, warning=FALSE}
df_month = df %>% group_by(month, hour_of_day) %>% summarize(mean_boarding = mean(boarding), mean_alighting = mean(alighting), mean_temp = mean(temperature))

months = c('Sep', 'Oct', 'Nov')

df_month$month = factor(df_month$month, levels=months)
df_month = df_month[order(df_month$month), ]

Legend = rep("Boarding", 48)
alighting_legend = rep("Alighting", 48)
temperature_legend = rep("Temperature", 48)

ggplot(df_month, aes(hour_of_day)) + geom_line(aes(y=mean_boarding, color=Legend), group=1) + geom_line(aes(y=mean_alighting, color=alighting_legend), group=1) +  geom_line(aes(y=mean_temp, color=temperature_legend), group=1) + labs(x = "Month", y = "Mean number of people", title = "Comparison of boarding vs alighting means by month and hour of day") + facet_grid(~month, scale='free_y')
```

```{r}
df_week = df %>% group_by(day_of_week) %>% summarise(mean_boarding = mean(boarding), mean_alighting = mean(alighting))
day_labs = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun") 
df_week$day_of_week = factor(df_week$day_of_week, levels= day_labs)
df_week = df_week[order(df_week$day_of_week), ]
```

```{r plot10, fig.cap="As we can see, the number of people boarding and alighting near campus on weekends are much lower.", fig.height=3}
Legend = rep("Boarding", 7)
legend_alighting = rep("Alighting", 7)
ggplot(df_week, aes(day_of_week)) + geom_line(aes(y=mean_boarding, color = Legend), group=1) + geom_line(aes(y=mean_alighting, color = legend_alighting), group=1) + labs(x = "Day of week", y = "Mean number of people", title = "Comparison of boarding vs alighting means by day of week")
```
```{r plot11, fig.cap="Similar patterns are seen across months, however significantly less people seem to attend classes on Monday in September.", fig.height=3, message=FALSE, warning=FALSE}
df_mweek = df %>% group_by(month, day_of_week) %>% summarize(mean_boarding = mean(boarding), mean_alighting = mean(alighting), mean_temp = mean(temperature))

day_labs = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun") 
df_mweek$day_of_week = factor(df_mweek$day_of_week, levels= day_labs)
df_week = df_week[order(df_mweek$day_of_week), ]

months = c('Sep', 'Oct', 'Nov')

df_mweek$month = factor(df_mweek$month, levels=months)
df_month = df_month[order(df_mweek$month), ]

Legend = rep("Boarding", 21)
alighting_legend = rep("Alighting", 21)
temperature_legend = rep("Temperature", 21)

ggplot(df_mweek, aes(day_of_week)) + geom_line(aes(y=mean_boarding, color=Legend), group=1) + geom_line(aes(y=mean_alighting, color=alighting_legend), group=1) +  geom_line(aes(y=mean_temp, color=temperature_legend), group=1) + labs(x = "Month", y = "Mean number of people", title = "Comparison of boarding vs alighting means by month and day of week") + facet_grid(~month, scale='free_y') + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r plot12, fig.cap="There seem to be more students coming in October compared to November and September. This is because of Thanksgiving holidays in November and since college starts in August, students might not immediately be using the buses on a regular basis in September.", fig.height=3, warning=FALSE, message=FALSE}
months = c('Sep', 'Oct', 'Nov')

df_month = df %>% group_by(month) %>% summarize(mean_boarding = mean(boarding), mean_alighting = mean(alighting))
df_month$month = factor(df_month$month, levels=months)
df_month = df_month[order(df_month$month), ]

Legend = rep("Boarding", 3)
alighting_legend = rep("Alighting", 3)

ggplot(df_month, aes(month)) + geom_line(aes(y=mean_boarding, color=Legend), group=1) + geom_line(aes(y=mean_alighting, color=alighting_legend), group=1) + labs(x = "Month", y = "Mean number of people", title = "Comparison of boarding vs alighting means by month") + theme(legend.position = "right")
```

\section{5. Portfolio modeling}

```{r include=FALSE}

library(ggstance)
library(mosaic)
library(quantmod)
library(foreach)
```

\subsection{Introduction}

We chose the below ETFs to provide portfolio diversification and a range of risk.

\begin{itemize}
\item Invesco (QQQ): One of the biggest, exclusively non-financial stock, and heavily tech-heavy trusts.
\item SPY: One of the safest and largest ETF.
\item iShares Russell 1000 Growth ETF (IWF): One of the most popular US large-cap growth ETFs with a long track record. It is an aggressive growing ETF.
\item SCO: A low performing stock.
\end{itemize}

In total, we have selected 3 ETFs - "QQQ", "SPY", "IWF". We looked at data from ETFs for five years commencing on January 1, 2017

```{r}
# Import the stocks
mystocks = c("QQQ", "SPY","IWF","SCO")

# Getting the price data for 5 years
myprices = getSymbols(mystocks, from='2017-01-01')
```


```{r include=FALSE}
#adjusting all stocks
for(ticker in mystocks){
  expr = paste0(ticker, "a=adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

```

\subsection{Sample Data for QQQ}
```{r echo=FALSE}
knitr::kable(head(QQQa, 10), caption="QQQ Sample Data")
```

\subsection{Combine all the returns in a matrix}
```{r,echo=FALSE}
# Computing the return matrix
all_returns = cbind( ClCl(QQQa),
                     ClCl(SPYa),
                     ClCl(IWFa),
                     ClCl(SCOa))
# Remove NAs
all_returns = as.matrix(na.omit(all_returns))
knitr::kable(head(all_returns, 10), caption="All Returns")

```

\subsection{Compute the returns from the closing prices}
```{r plot13, fig.cap= "We can see a strong linear correlation here in 3 of the stocks QQQ, SPY and IWF. SCO seems to not have a strong correlation with the other stocks", fig.align='left', fig.height = 2.5, fig.width = 5} 
# Compute the returns from the closing prices
pairs(all_returns)
```

\subsection{Volatility of the ETFs}

```{r plot14, fig.cap="Volatility of the ETFs across the 5 year period."}

par(mfrow=c(2, 2))
plot(ClCl(QQQa), type='l')
plot(ClCl(SPYa), type='l')
plot(ClCl(IWFa), type='l')
plot(ClCl(SCOa), type='l')
```

\subsection{Sample a random return from the empirical joint distribution}

We simulated a random day's return.
```{r}
return.today = resample(all_returns, 1, orig.ids=FALSE)
return.today
```

\subsection{SIMULATION 1: LOW RISK PORTFOLIO}

\begin{itemize}
\item \textbf{INITIAL INVESTMENT IS \$ 100000.}
\item Average return of investment after 20 days  = \$ 101190.
\item 5\% value at Risk for safe portfolio = \$ 12139.7.
\end{itemize}

Now simulate many different possible futures, repeating the above block thousands of times
```{r}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	  total_wealth = initial_wealth
  	weights = c(0.25, 0.25, 0.25, 0.25)
  	holdings = weights * total_wealth
  	n_days = 20
  	wealthtracker = rep(0, n_days)
  	for(today in 1:n_days) {
		  return.today = resample(all_returns, 1, orig.ids=FALSE)
	  	holdings = holdings + holdings*return.today
	  	total_wealth = sum(holdings)
	  	wealthtracker[today] = total_wealth
	}
	wealthtracker
}
head(sim1, 10)

```

```{r plot15, fig.cap="Histogram and density plots of simulation 1. We see spikes at 100000" ,fig.align='left', fig.height = 3.5, fig.width = 4}
hist(sim1[,n_days], 25)
plot(density(sim1[,n_days]))
```

\subsection{Profit and Loss}

\begin{itemize}
\item Average return of investment after 20 days = \$101099.5
\item Average profit/loss after 20 days \$1099.513 
\end{itemize}

```{r plot16, message=FALSE, fig.cap="Histogram of returns of simulation 1.", fig.height=3.5, results='hide'}
# Profit/loss
cat('\nAverage return of investment after 20 days', mean(sim1[,n_days]), "\n")
cat('\nAverage profit/loss after 20 days', mean(sim1[,n_days] - initial_wealth), "\n")

hist(sim1[,n_days]- initial_wealth, breaks=30)
```
\textbf{5\% Value at Risk for the first simulation = -12293.44}
```{r message=FALSE, results='hide'}
# 5% value at risk:
a=quantile(sim1[,n_days]- initial_wealth, prob=0.05)
cat('\n5% Value at Risk for the first simulation-',a, "\n")
```
\subsection{SIMULATION 2: HIGH RISK PORTFOLIO}

\begin{itemize}
\item Average return of investment after 20 days = \$101576.2 
\item 5\% Value at Risk for safe portfolio = \$7849.9327
\end{itemize}

```{r}
initial_wealth = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
	  total_wealth = initial_wealth
  	weights = c(0.2, 0.2, 0.5, 0.1)
  	holdings = weights * total_wealth
  	n_days = 20
  	wealthtracker = rep(0, n_days)
  	for(today in 1:n_days) {
		  return.today = resample(all_returns, 1, orig.ids=FALSE)
	  	holdings = holdings + holdings*return.today
	  	total_wealth = sum(holdings)
	  	wealthtracker[today] = total_wealth
	}
	wealthtracker
}
head(sim2, 10)
```

```{r plot17, fig.cap='Histogram and density plots of simulation 2. We see spikes at 100000', fig.align='left', fig.height = 3.5, fig.width = 4}
hist(sim2[,n_days], 25)
plot(density(sim2[,n_days]))
```

\subsection{Profit and Loss}
```{r plot18, fig.cap="Histogram of returns of simulation 2.", fig.height=3.5, results='hide'}
# Profit/loss
cat('\nAverage return of investment after 20 days', mean(sim2[,n_days]), "\n")
cat('\nAverage profit/loss after 20 days', mean(sim2[,n_days] - initial_wealth), "\n")

hist(sim2[,n_days]- initial_wealth, breaks=30)
```

\textbf{5\% Value at Risk for the second simulation = -8281.054}
```{r, message=FALSE, results='hide'}
# 5% value at risk:
a=quantile(sim2[,n_days]- initial_wealth, prob=0.05)
cat('\n5% Value at Risk for the second simulation-',a, "\n")
```

\subsection{Summary}

For the safe portfolio, we are observing lower return of investment and lower 5\% VaR. 

As the portfolio risk increased, we are able to witness the increase in returns and an increase in VaR value as expected. 

References:
https://www.bankrate.com/investing/best-etfs/
https://etfdb.com/compare/lowest-ytd-returns/


\section{Clustering and PCA}
```{r include=FALSE}
rm(list = ls())

library(tidyverse)
library(dplyr)
library(ggplot2)
library("RColorBrewer")
library(magrittr)
```


**Sample Data**
```{r echo=FALSE}
wine <- read.csv('wine.csv')
 head(wine)

```

**Data Summary**
```{r echo=FALSE}
summary(wine)
```

** PCA using prcomp that uses the singular value decomposition (SVD)**
```{r echo=FALSE}
# PCA
PCAwine <- prcomp(wine[,-(12:13)], scale=TRUE)
summary(PCAwine)
```
** Interpreting the components **
```{r echo=FALSE}
round(PCAwine$rotation[,1:2],2)
```

**Summary of the loadings**
```{r echo=FALSE}
# create a tidy summary of the loadings
loadings_summary = PCAwine$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Properties')

loadings_summary
```

```{r echo=FALSE}
# PC1 seems to give more positive loadings to sulfur dioxide and residual sugar while chlorides, sulphates & acidity have negative loadings
loadings_summary %>%
  select(Properties, PC1) %>%
  arrange(desc(PC1)) %>%
  mutate(PC1 = round(PC1,2))
```

** PC1 seems to give more positive loadings to sulfur dioxide and residual sugar while chlorides, sulphates & acidity have negative loadings**

```{r echo=FALSE}
# PC2 gives more positive loading to density and high negative to alcohol content
loadings_summary %>%
  select(Properties, PC2) %>%
  arrange(desc(PC2)) %>%
  mutate(PC2 = round(PC2,2))
```
**PC2 gives more positive loading to density and high negative to alcohol content**

```{r echo=FALSE}
# Let's make some plots of the shows themselves in 
# PC space, i.e. the space of summary variables we've created
wine = merge(wine, PCAwine$x[,1:2], by="row.names")
wine = rename(wine, Show = Row.names)

```
** PC space, i.e. the space of summary variables we've created**

```{r echo=FALSE}
myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))

```

```{r echo=FALSE}
wine %>% ggplot(aes(x = PC1, y = PC2)) + geom_point()
# We can see that there are two clusters emerging. One to the left having more negative PC1 & another to the right having more positive PC1
# As we do not have access to the wine color, we cannot say exactly which of these two clusters correspond to red & White wine
# However, the difference in the loadings for each feature by the principal components implies that these two clusters are distinct

```

* We can see that there are two clusters emerging. One to the left having more negative PC1 & another to the right having more positive PC1
* As we do not have access to the wine color, we cannot say exactly which of these two clusters correspond to red & White wine
* However, the difference in the loadings for each feature by the principal components implies that these two clusters are distinct


**Categorize quality **
```{r echo=FALSE}
# Predicting quality
# Categorize quality 
wine$Q_score <- NA
wine[wine$quality <= 5,]$Q_score <- 'Low Quality'
wine[wine$quality > 5,]$Q_score <- 'High Quality'
table(wine$Q_score)
```

```{r echo=FALSE}
wine %>% ggplot(aes(x = PC1, y = PC2, col = Q_score)) + geom_point()

```

**It doesn't look like the principal components can do a good job of separating high & low quality wines.**

```{r echo=FALSE}
# k-Means clustering
# Center and scale the data
wine = read.csv('wine.csv')
X = wine[,-(12:14)]
X = scale(X, center=TRUE, scale=TRUE)

```


```{r echo=FALSE}
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

```{r echo=FALSE}
# Run k-means with 2 clusters and 25 starts
clust1 = kmeans(X, 2, nstart=25)

```

**Clusters**

```{r echo=FALSE}
# What are the clusters?
clust1$center[1,]*sigma + mu # More sulfur dioxide & sugar compared to cluster 2
clust1$center[2,]*sigma + mu # More acidity, chlorides & sulphates compared to cluster 1

```



```{r echo=FALSE}
sort(clust1$center[1,] - clust1$center[2,])
```
**Table clusters**

```{r echo=FALSE}
table(clust1$cluster, wine$color)
```

```{r echo=FALSE}
wine$dist1 <- sqrt(rowSums((sweep(wine[,-(12:13)], 2, (clust1$center[1,] *sigma + mu)))^2))
wine$dist2 <- sqrt(rowSums((sweep(wine[,-(12:14)], 2, (clust1$center[2,]*sigma + mu)))^2))

head(wine)

```

**Using the two most contrasting features for the 2 clusters as x & y to visualize**
```{r echo=FALSE}
# Using the two most contrasting features for the 2 clusters as x & y to visualize
wine %>% ggplot(aes(x = total.sulfur.dioxide, y = volatile.acidity, col = factor(clust1$cluster))) + geom_point()
# The 2 clusters are pretty well separated. And as these two clusters differ widely in their chemical characteristics,
# we can say that k-Means can differentiate red from white wine.

```

**PLot for High and Low Quality Wine**
```{r echo=FALSE}
wine$Q_score <- NA
wine[wine$quality <= 5,]$Q_score <- 'Low Quality'
wine[wine$quality > 5,]$Q_score <- 'High Quality'
wine %>% ggplot(aes(x = total.sulfur.dioxide, y = volatile.acidity, col = factor(Q_score))) + geom_point()

```

```{r echo=FALSE}
wine %>% ggplot(aes(x = dist1, y = dist2, col = color)) +
  geom_point() +
  facet_grid(cols = vars(color))

```
```{r echo=FALSE}
wine %>% ggplot(aes(x = dist1, y = dist2, col = color)) +
  geom_point()

```

```{r echo=FALSE}
wine %>% ggplot(aes(x = dist1, y = dist2, col = quality)) +
  geom_point() +
  facet_grid(cols = vars(quality))

```

```{r echo=FALSE}
wine %>% ggplot(aes(x = dist1, y = dist2, col = quality)) +
  geom_point() + scale_colour_gradientn(colours = myPalette(100), limits=c(3, 9))

```

** Summary**

* PCA is the best dimensionality reduction technique makes more sense to me for this data
* PCA Counteracts the issues of high-dimensional data
* PCA improves performance at a very low cost of model accuracy

Correlation Plot
```{r, include=FALSE}
library(ggplot2)
library(ggthemes)
library(cluster)
library(fpc)
library(reshape2)
library(RCurl)
library(foreach)
library('corrplot')


soical_marketing_file = 'social_marketing.csv'
social_marketing_raw <- read.csv(soical_marketing_file)
social_marketing <- read.csv(soical_marketing_file)
```

```{r}
cor = round(cor(social_marketing_raw[,2:37]), 2)
corrplot(cor, method="square")
```

The plot at first glance will just show us what how many and what variables are showing a correlation. We see that many of the variables are correlated. For example, the plot is showing us that college uni and online gaming are correlated. We also see that health nutrition and personal fitness are correlated as well.

We can use the principal component analysis to summarize the information content in large data tables by means of a smaller set of “summary indices” that can be more easily visualized and analyzed.

```{r}
social_marketing$chatter<- NULL
social_marketing$spam <- NULL
social_marketing$health_nutrition <- NULL 
social_marketing$photo_sharing <- NULL
social_marketing$adult <- NULL

scaled_var = social_marketing[,(2:32)]
scaled_var = scale(scaled_var, center=TRUE, scale=TRUE)

mu = attr(scaled_var,"scaled:center")
sigma = attr(scaled_var,"scaled:scale")
```

We've centered and scaled the data and extracted them which will be the attributes. 

```{r}

pca = prcomp(social_marketing_raw[,2:32], scale=TRUE, center = TRUE)

summary(pca)

pca_1 <-  pca$sdev ^ 2
pca_2 <- pca_1 / sum(pca_1)

plot(cumsum(pca_2), xlab = "Principal Component", 
     ylab = "Variance Explained")
cumsum(pca_2)[10]
```

Looking at the 10th principal component we can see that roughly 64.6% of the fraction of variance is explained. Using the Kaiser Criterion we can drop all Eigen values under 1.0. 

```{r}
set.seed(123)

k.max = 15
data = scaled_var

cluster1 = kmeans(scaled_var, 10, nstart=25)

social_cluster1 = cbind(social_marketing, cluster1$cluster)

plotcluster(social_marketing[,2:32], cluster1$cluster)
```

Using the elbow method we've determine the number of clusters. We've decided to move forward with 10 clusters. 


```{r}

social_cluster1_main = as.data.frame(cbind(cluster1$center[1,]*sigma + mu, 
                                          cluster1$center[2,]*sigma + mu,
                                          cluster1$center[3,]*sigma + mu,
                                          cluster1$center[4,]*sigma + mu,
                                          cluster1$center[5,]*sigma + mu,
                                          cluster1$center[6,]*sigma + mu,
                                          cluster1$center[7,]*sigma + mu,
                                          cluster1$center[8,]*sigma + mu,
                                          cluster1$center[9,]*sigma + mu,
                                          cluster1$center[10,]*sigma + mu))

summary(social_cluster1_main)

names(social_cluster1_main) <- c('Cluster_1',
                               'Cluster_2',
                               'Cluster_3',
                               'Cluster_4',
                               'Cluster_5',
                               'Cluster_6',
                               'Cluster_7',
                               'Cluster_8',
                               'Cluster_9',
                               'Cluster_10')

```

```{r}
social_cluster1_main$type = row.names(social_cluster1_main)
#Cluster 1
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 1",
       x ="Category", y = "Centre Values")
```
Cluster 1 primarily composed with people who have interest in current_events, traveling and politics. 

```{r}
#Cluster 2 
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 2",
       x ="Category", y = "Cluster Centre")
```
Cluster 2 primarily composed with people who have interest in shopping, current_events, college_uni. 

```{r}
#Cluster 3
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 3",
       x ="Category", y = "Cluster Centre")
```
Cluster 3 emphasized people who have interest in dating then followed by fashion.

```{r}
#Cluster 4
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 4",
       x ="Category", y = "Cluster Centre")
```
Cluster 4 primarily composed of people who have interest in sports_fandom, relition, and parenting.


```{r}
#Cluster 5
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_5) , y=Cluster_5)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 5",
       x ="Category", y = "Cluster Centre")
```
Cluster 5 primarily composed of people who have interest in cooking, fashion and beauty. 


```{r}
#Cluster 6
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_6) , y=Cluster_6)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 6",
       x ="Category", y = "Cluster Centre")
```
Cluster 6 primarily composed of people who have interest in college_unit, online gaming and sports_playing. 

```{r}
#Cluster 7
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_7) , y=Cluster_7)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 7",
       x ="Category", y = "Cluster Centre")
```
Cluster 7 primarily composed of people who have interest in art, tv_film, and current_events. 

```{r}
#Cluster 8
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_8) , y=Cluster_8)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 8",
       x ="Category", y = "Cluster Centre")
```
Cluster 8 primarily composed of people who have interest in news, politics, and automotive. 


```{r}
#Cluster 9
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_9) , y=Cluster_9)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 9",
       x ="Category", y = "Cluster Centre")
```
Cluster 9 primarily composed of people who have interest in politics, travel, computers. 

```{r}
#Cluster 10
ggplot(social_cluster1_main, aes(x =reorder(type, -Cluster_10) , y=Cluster_10)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_economist() + 
  theme(axis.text.x = element_text(angle=-42, hjust=.2)) + 
  labs(title="Cluster 10",
       x ="Category", y = "Cluster Centre") 
```
Cluster 10 primarily composed of people who have interest in personal_fitness, cooking, outdoors.

K-Means clustering can allow us to identify the different market segments. In this case, we used 10 clusters to describe the unique market segments that the dataset has. These can then be used to create marketing campaigns. 
